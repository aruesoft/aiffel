{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "621d43b8-5c72-4415-97b2-a892d916efa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(784, 50) (50,) (50, 10) (10,)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# mnist를 불러오고 train_data, train_label, test_data, test_label로 나눠주세요.\n",
    "mnist = keras.datasets.mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()   \n",
    "\n",
    "\n",
    "# 모델에 맞게 데이터 가공\n",
    "x_train_norm, x_test_norm = x_train / 255.0, x_test / 255.0\n",
    "x_train_reshaped = x_train_norm.reshape(-1, x_train_norm.shape[1]*x_train_norm.shape[2])\n",
    "x_test_reshaped = x_test_norm.reshape(-1, x_test_norm.shape[1]*x_test_norm.shape[2])\n",
    "\n",
    "# 입력층 데이터의 모양(shape)\n",
    "print(x_train_reshaped.shape)\n",
    "\n",
    "# 테스트를 위해 x_train_reshaped의 앞 5개의 데이터를 가져온다.\n",
    "X = x_train_reshaped[:]\n",
    "\n",
    "# 초기화된 파라미터를 정의하는 함수를 만들고 초기값을 만드세요.\n",
    "def init_params(input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "    #W1, b1, W2, b2를 모두 정의해주세요.\n",
    "    \n",
    "    # 인접 레이어간 관계를 나타내는 파라미터 W를 생성하고 random 초기화\n",
    "    W1 = weight_init_std * np.random.randn(input_size, hidden_size)  \n",
    "    # 바이어스 파라미터 b를 생성하고 Zero로 초기화\n",
    "    b1 = np.zeros(hidden_size)\n",
    "    \n",
    "    W2 = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "    b2 = np.zeros(output_size)\n",
    "    \n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "W1, b1, W2, b2 = init_params(input_size = 784, hidden_size = 50, output_size = 10)\n",
    "print(W1.shape,b1.shape,W2.shape,b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2222128-8943-4a7c-ac46-409c0675e7d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MLP를 정의하세요.\n",
    "def affine_layer_forward(X, W, b):\n",
    "    y = np.dot(X, W) + b\n",
    "    cache = (X, W, b)\n",
    "    return y, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1da6e7e3-cbf4-485d-a277-ee9decc77f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu를 정의하세요 (np.maximum을 활용하세요)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c534001-67c0-426a-ad15-d6c9826d8182",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2539d241-09c9-4627-b582-183f68938a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# softmax를 정의하세요\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x))\n",
    "        return y.T\n",
    "    x = x - np.max(x)\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3fb355a-fcf3-40f7-93b5-1101aa7cf417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot 인코딩을 정의하세요\n",
    "def _change_one_hot_label(X, num_category):\n",
    "    T = np.zeros((X.size, num_category))\n",
    "    for idx, row in enumerate(T):\n",
    "        row[X[idx]] = 1\n",
    "        \n",
    "    return T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1990e1-dfef-4295-8b97-e8156e2d12ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58da1dce-e149-4476-bf00-374a51e53c7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MLP의 backward pass를 정의하세요\n",
    "def affine_layer_backward(dy, cache):\n",
    "    X, W, b = cache\n",
    "    dX = np.dot(dy, W.T)\n",
    "    dW = np.dot(X.T, dy)\n",
    "    db = np.sum(dy, axis=0)\n",
    "    return dX, dW, db\n",
    "\n",
    "\n",
    "# relu 함수의 backward pass를 정의하세요. (np.where 함수를 활용하세요)\n",
    "\n",
    "def relu_grad(x):\n",
    "        \n",
    "    return (1.0 - relu(x) ) * relu(x) \n",
    "\n",
    "\n",
    "#파라미터를 업데이트하는 함수를 정의하세요.\n",
    "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate):\n",
    "    W1 = W1 - learning_rate*dW1\n",
    "    b1 = b1 - learning_rate*db1\n",
    "    W2 = W2 - learning_rate*dW2\n",
    "    b2 = b2 - learning_rate*db2\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "# train_step을 정의합니다.\n",
    "def train_step(X, Y, W1, b1, W2, b2, learning_rate=0.1, verbose=False):\n",
    "    a1, cache1 = affine_layer_forward(X, W1, b1)\n",
    "    # z1 = relu(a1)\n",
    "    z1 = relu(a1)\n",
    "    a2, cache2 = affine_layer_forward(z1, W2, b2)\n",
    "    y_hat = softmax(a2)\n",
    "    t = _change_one_hot_label(Y, 10)\n",
    "    Loss = cross_entropy_error(y_hat, t)\n",
    "\n",
    "    if verbose:\n",
    "        print('---------')\n",
    "        print(y_hat)\n",
    "        print(t)\n",
    "        print('Loss: ', Loss)\n",
    "        \n",
    "    dy = (y_hat - t) / X.shape[0]\n",
    "    dz1, dW2, db2 = affine_layer_backward(dy, cache2)\n",
    "    #da1 = relu_grad(a1) * dz1\n",
    "    da1 = relu_grad(a1) * dz1\n",
    "    dX, dW1, db1 = affine_layer_backward(da1, cache1)\n",
    "    \n",
    "    W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n",
    "    \n",
    "    return W1, b1, W2, b2, Loss\n",
    "\n",
    "def predict(W1, b1, W2, b2, X):\n",
    "    a1 = np.dot(X, W1) + b1\n",
    "  #  z1 = relu(a1)\n",
    "    z1 = relu(a1)\n",
    "    a2 = np.dot(z1, W2) + b2\n",
    "    y = softmax(a2)\n",
    "\n",
    "    return y\n",
    "\n",
    "def accuracy(W1, b1, W2, b2, x, y):\n",
    "    y_hat = predict(W1, b1, W2, b2, x)\n",
    "    y_hat = np.argmax(y_hat, axis=1)\n",
    "\n",
    "    accuracy = np.sum(y_hat == y) / float(x.shape[0])\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "298f16a9-2761-4870-aeef-9e611d00027c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  13.304728080105994\n",
      "train acc, test acc | 0.07191666666666667, 0.068\n",
      "Loss:  13.344171243250965\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  13.490346331898738\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  13.823689310361036\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  14.275789675512016\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  14.83531393260901\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  15.46548973457011\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  16.13745807768338\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  16.832713414780486\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  17.540695118719484\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  18.255621113352905\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  18.974377112271753\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  19.695268682324915\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  20.41738713580052\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  21.140199776935027\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  21.86342072632564\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  22.586882091581735\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  23.310486890220446\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  24.03417731826594\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  24.757919511100063\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  25.481718050437934\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  26.206229083989275\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  26.929289976885254\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  27.653101380113362\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  28.376917175327264\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  29.10073565043154\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  29.824555781175942\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  30.548376931934317\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  31.27219871219773\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  31.99602088151891\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  32.719843291017156\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  33.44366584921348\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  34.16748849938886\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  34.891311206532016\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  35.61513394896454\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  36.33895671326384\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  37.06277949112064\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  37.78660227737753\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  38.51042506884996\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  39.234247863548454\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  39.95807066025778\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  40.6818934582147\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  41.405716256944594\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  42.129539056155224\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  42.85336372426141\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  43.577184655366956\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  44.30100745518139\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  45.0248302550673\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  45.748653054998755\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  46.4724758549581\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  47.19629865493456\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  47.92012145492164\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  48.64394425491533\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n",
      "Loss:  49.367767054913124\n",
      "train acc, test acc | 0.11236666666666667, 0.1135\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m x_batch \u001b[38;5;241m=\u001b[39m x_train_reshaped[batch_mask]\n\u001b[1;32m     20\u001b[0m y_batch \u001b[38;5;241m=\u001b[39m y_train[batch_mask]\n\u001b[0;32m---> 22\u001b[0m W1, b1, W2, b2, Loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 학습 경과 기록\u001b[39;00m\n\u001b[1;32m     25\u001b[0m train_loss_list\u001b[38;5;241m.\u001b[39mappend(Loss)\n",
      "Cell \u001b[0;32mIn[7], line 46\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(X, Y, W1, b1, W2, b2, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#da1 = relu_grad(a1) * dz1\u001b[39;00m\n\u001b[1;32m     45\u001b[0m da1 \u001b[38;5;241m=\u001b[39m relu_grad(a1) \u001b[38;5;241m*\u001b[39m dz1\n\u001b[0;32m---> 46\u001b[0m dX, dW1, db1 \u001b[38;5;241m=\u001b[39m \u001b[43maffine_layer_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mda1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, learning_rate)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m W1, b1, W2, b2, Loss\n",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m, in \u001b[0;36maffine_layer_backward\u001b[0;34m(dy, cache)\u001b[0m\n\u001b[1;32m      3\u001b[0m X, W, b \u001b[38;5;241m=\u001b[39m cache\n\u001b[1;32m      4\u001b[0m dX \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(dy, W\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m----> 5\u001b[0m dW \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dy, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dX, dW, db\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 하이퍼파라미터\n",
    "iters_num = 50000  # 반복 횟수를 적절히 설정한다.\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100   # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "W1, b1, W2, b2 = init_params(784, 50, 10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train_reshaped[batch_mask]\n",
    "    y_batch = y_train[batch_mask]\n",
    "  \n",
    "    W1, b1, W2, b2, Loss = train_step(x_train_reshaped, y_train, W1, b1, W2, b2,learning_rate=0.1, verbose=False)\n",
    "\n",
    "    # 학습 경과 기록\n",
    "    train_loss_list.append(Loss)\n",
    "  \n",
    "    # 1에폭당 정확도 계산\n",
    "    # train_accuracy와 test_accuracy를 완성해주세요\n",
    "    if i % iter_per_epoch == 0:\n",
    "        print('Loss: ', Loss)\n",
    "        train_acc = accuracy(W1, b1, W2, b2, x_train_reshaped, y_train)\n",
    "        test_acc = accuracy(W1, b1, W2, b2, x_test_reshaped, y_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294fdd8-fef3-4b84-bac3-2d2541a5748e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
